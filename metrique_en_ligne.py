# -*- coding: utf-8 -*-
"""metrique_en_ligne.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i7nOr3bHQaTwDrIh-adD0yxh-mZ2D3KY
"""
# !pip install -q requests-html tqdm
import json
import time
from collections import Counter
from pathlib import Path
from urllib import parse

from requests_html import HTML
from requests_html import HTMLSession
from tqdm.auto import tqdm


session = HTMLSession()
base_url = "https://crisco2.unicaen.fr/verlaine/index.php?navigation=textesanalyses&choix_liste=poemes"
base_response = session.get(base_url)
authors = base_response.html.find("#constrainer2 > table:nth-child(1) > tbody:nth-child(2) > tr > td:nth-child(1) > a:nth-child(1)")

# !mkdir corpora
visited = set()
for author in tqdm(authors, "Authors"):
    if author.absolute_links:
        works_url = list(author.absolute_links)[0]
        if works_url in visited:
            continue
        else:
            visited.add(works_url)
        works_response = session.get(works_url)
        works = works_response.html.find(".ref_ouvrage > a")
        for workd_id, work in enumerate(tqdm(works, "Works", leave=False)):
            work_url = list(work.absolute_links)[0]
            # print("WORK", work_url)
            if work_url in visited:
                continue
            else:
                visited.add(work_url)
            parts_response = session.get(work_url)
            parts = parts_response.html.find(".ref_ouvrage > a")
            texts = {}
            author_code = None
            for part in tqdm(parts, "Texts", leave=False):
                part_url = list(part.absolute_links)[0]
                if "code_text=" in part_url:
                    query = parse.urlsplit(part_url).query
                    params = dict(parse.parse_qsl(query))
                    author_code = params["auteur"]
                    text_code = params["code_text"]
                    # URL ending with "_7" for rhyme and stanzas
                    params["code_text"] = f"{params['code_text']}_7"
                    part_url = part_url.replace(query, parse.urlencode(params))
                    if part_url in visited:
                        continue
                    else:
                        visited.add(part_url)
                    part_response = session.get(part_url)
                    try:
                        texts_html = part_response.html.find(
                            "#textes", first=True
                        ).html
                    except AttributeError:
                        print(part_url)
                        continue
                    texts[text_code] = {
                        "url": part_url,
                        "html": texts_html,
                    }
                    time.sleep(0.2)
            if texts and author_code:
                filename = f"corpora/{author_code}-{workd_id + 1}.json"
                with open(filename, "w") as raw:
                    json.dump(texts, raw, indent=4)

# !mkdir jsons
corpus = []
for json_file in tqdm(list(Path("corpora").rglob("*.json"))):
    author_works = []
    author_code = json_file.name.split("-")[0].split("_")[0]
    pairs = json.loads(json_file.open().read()).values()
    for pair in tqdm(pairs, author_code, leave=False):
        html = pair["html"]
        url = pair["url"]
        chunk = HTML(html=html)
        code = chunk.find(".div_code_poeme", first=True).text
        author = chunk.find(".div_auteur", first=True).text
        work = chunk.find(".div_recueil_titre_main", first=True).text
        date = chunk.find(".div_date", first=True).text
        title_main = chunk.find(".td_titre_main", first=True)
        title_num = chunk.find(".td_titre_num", first=True)
        if title_main:
            title = title_main.text
        elif title_num:
            title = title_num.text
        else:
            title = work
        profile = chunk.find(".td_profil > .span_infos_analyse_contenu",
            first=True).text
        structure = chunk.find(".td_forme > .span_infos_analyse_contenu",
            first=True).text
        work = {
            "code": code, "author": author, "work": work,
            "date": date, "title": title, "profile": profile,
            "structure": structure, "url": url,
        }
        stanzas = []
        for stanza in chunk.find(".table_lg"):
            verses = [s.text for s in stanza.find(".td_vers")]
            metres = [s.text for s in stanza.find("td[class^='td_met']")]
            rhymes = [s.text for s in stanza.find("td[class^='td_rime']")]
            stanzas.append([
                {"verse": verse, "metre": metre, "rhyme": rhyme}
                for verse, metre, rhyme in zip(*[verses, metres, rhymes])
            ])
        work["text"] = stanzas
        author_works.append(work)
        corpus.append(work)
    with open(f"jsons/{author_code}.json", "w") as author_json:
        json.dump(author_works, author_json, indent=4)
with open(f"metrique_en_ligne.json", "w") as corpus_json:
    json.dump(corpus, corpus_json, indent=4)


from collections import Counter

counter = Counter()
line_counter = 0
stanza_counter = 0
word_counter = 0
for work in corpus:
    for stanza in work["text"]:
        stanza_counter += 1
        for line in stanza:
            counter.update([len(line["verse"])])
            line_counter += 1
            word_counter += len(line["verse"].split())
print("Works", len(corpus))
print("Stanzas", stanza_counter)
print("Words", word_counter)
print("Verses", line_counter)
print("Characters", sum(k * v for k, v in counter.items()))
